{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f77a8837",
   "metadata": {},
   "source": [
    "# HW2\n",
    "\n",
    "Jaee Oh / Tejas Phanse / Mandadi Ayush Reddy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899e554",
   "metadata": {},
   "source": [
    "## Part A: DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16840be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jaee/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/jaee/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') \n",
    "print(\"Libraries loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b20fa",
   "metadata": {},
   "source": [
    "- Task1: Text Prerpcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edb54dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# List of common words to be removed from full_text_clean column\n",
    "common_word_list = {'followers', 'instagram', 'facebook', 'like', 'messages', 'support',\n",
    "                    'sitemap', 'free', 'faq', 'twitter', 'search', 'subscribe', 'youtube', \n",
    "                    'feedback', 'linkedin', 'demo', 'menu', 'following', 'unsubscribe', \n",
    "                    'snapchat', 'member', 'tiktok', 'whatsapp', \n",
    "                    'profile', 'share', 'reddit', 'download', 'settings', 'notifications', \n",
    "                    'home', 'qa', 'people'}\n",
    "stop_words = stop_words.union(common_word_list) # Add common words to NLTK stop words\n",
    "\n",
    "# Common phrases to remove\n",
    "phrase_list = [\"find out more\", \"join now\",\"terms & conditions\", \"terms and conditions\",\"all rights reserved\",\"free trial\", \n",
    "             \"click here\", \"about us\", \"contact us\",\"terms of service\", \"privacy policy\", \"help center\", \"our story\",\"our team\",\n",
    "             \"read more\", \"learn more\", \"get started\", \"download now\", \"sign up\", \"log in\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove \\n, \\r, \\t\n",
    "    text = re.sub(r'[\\n\\r\\t]+', ' ', text)\n",
    "\n",
    "    # Remove HTML tags like <body>, but first remove script/style content\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    for tag in soup(['script', 'style', 'noscript']):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text(separator=' ')\n",
    "    \n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'(?:https?://|ftp://|www\\.)\\S+|mailto:\\S+|tel:\\S+', '', text)\n",
    "    \n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\\b', '', text)\n",
    "    \n",
    "    # Remove phrases\n",
    "    for phrase in phrase_list:\n",
    "        text = text.replace(phrase, ' ')\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]', ' ', text)\n",
    "    \n",
    "    # Keep only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # Tokenize and filter with stop words and lemmatization\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if len(t) > 2 and t not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7a97eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    velvet mousse blush cream ebelebelle top page ...\n",
      "1    forge carter cfcc stone inlay david douglas di...\n",
      "2    travel news guide destination tip close update...\n",
      "3    find ridgerun solution ridgerun developer wiki...\n",
      "4    shooting ktvu fox live news weather sport cont...\n",
      "Name: text_clean, dtype: object\n",
      "Matrix shape: (25, 3216)\n",
      "  - 25 documents\n",
      "  - 3216 unique terms in vocabulary\n",
      "\n",
      "First 20 terms: ['abdicate' 'abduction' 'abdulaziz' 'abdullah' 'abiding' 'abingdon' 'able'\n",
      " 'abode' 'abounding' 'aboveboard' 'abril' 'abrupt' 'absolute' 'absolve'\n",
      " 'abstraction' 'academy' 'accept' 'acceptance' 'access' 'accessability']\n",
      "Last 20 terms: ['xact' 'xcelerator' 'xilinx' 'xlive' 'xshare' 'yanagita' 'year' 'yellow'\n",
      " 'yet' 'yieldenhancer' 'yocto' 'york' 'yorkshire' 'youre' 'youve' 'yuan'\n",
      " 'zebrafish' 'zfa' 'zip' 'zone']\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "documents = pd.read_csv('../data/dataset_for_BOW_Class.csv')\n",
    "documents['text_clean'] = documents['full_text'].apply(preprocess)\n",
    "print(documents['text_clean'].head())\n",
    "\n",
    "# Basic CountVectorizer\n",
    "count_vec = CountVectorizer()\n",
    "count_matrix = count_vec.fit_transform(documents['text_clean'])  # Fixed\n",
    "\n",
    "print(f\"Matrix shape: {count_matrix.shape}\")\n",
    "print(f\"  - {count_matrix.shape[0]} documents\")\n",
    "print(f\"  - {count_matrix.shape[1]} unique terms in vocabulary\")\n",
    "\n",
    "# Be careful with large vocabularies - this could print thousands of words\n",
    "vocab = count_vec.get_feature_names_out()\n",
    "print(f\"\\nFirst 20 terms: {vocab[:20]}\")\n",
    "print(f\"Last 20 terms: {vocab[-20:]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
